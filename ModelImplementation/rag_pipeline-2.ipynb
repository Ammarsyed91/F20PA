{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "wQarh2KnA9-l",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python langchain sentence-transformers openai==0.28.0 transformers torch \"accelerate>=0.26.0\" faiss-cpu ipywidgets gradio\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L4Q7QaNXA_6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the base path to your data files.\n",
        "# Update this if your folder name is different.\n",
        "BASE_PATH = '/content/drive/MyDrive/ColabNotebooks/Dissertataion'\n",
        "print(\"BASE_PATH is set to:\", BASE_PATH)\n"
      ],
      "metadata": {
        "id": "wN-i9duoBBlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename=\"faiss_errors.log\",\n",
        "    filemode=\"a\",\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    level=logging.ERROR\n",
        ")\n",
        "\n",
        "print(\"All packages imported and logging configured!\")\n",
        "print(\"NumPy version:\", np.__version__)\n"
      ],
      "metadata": {
        "id": "8YTnGkCkBD3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure BASE_PATH matches the folder containing your files:\n",
        "BASE_PATH = '/content/drive/MyDrive/ColabNotebooks/Dissertation'\n",
        "print(\"BASE_PATH is set to:\", BASE_PATH)\n",
        "\n",
        "# Load FAISS index:\n",
        "index = faiss.read_index(os.path.join(BASE_PATH, \"my_index.idx\"))\n",
        "print(\"FAISS index loaded from disk.\")\n",
        "\n",
        "# Load metadata files:\n",
        "with open(os.path.join(BASE_PATH, \"all_texts.pkl\"), \"rb\") as f:\n",
        "    all_texts = pickle.load(f)\n",
        "with open(os.path.join(BASE_PATH, \"doc_ids.pkl\"), \"rb\") as f:\n",
        "    doc_ids = pickle.load(f)\n",
        "with open(os.path.join(BASE_PATH, \"chunk_nums.pkl\"), \"rb\") as f:\n",
        "    chunk_nums = pickle.load(f)\n",
        "print(\"Metadata loaded from pickle files.\")\n"
      ],
      "metadata": {
        "id": "2iuetZ4mBFJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"Embedding model loaded.\")\n"
      ],
      "metadata": {
        "id": "KPthJFZcBHN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Testing using the pipeline helper:\n",
        "test_pipe = pipeline(\"text-generation\",\n",
        "                       model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "                       tokenizer=\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "test_output = test_pipe(\"Who are you?\", max_new_tokens=32, do_sample=True)\n",
        "print(\"Test output from the 1B model (pipeline):\")\n",
        "print(test_output[0][\"generated_text\"])\n",
        "\n",
        "# Alternatively, load the model directly (if you need more control):\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "llama_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
        "print(\"Direct loading of the Llama 1B model successful!\")\n"
      ],
      "metadata": {
        "id": "1vl9Dh7yBIo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(user_query):\n",
        "    # Generate an embedding for the user query using the embedding model\n",
        "    query_embedding = embedding_model.encode([user_query]).astype(\"float32\")\n",
        "\n",
        "    # Retrieve the top k similar texts from the FAISS index\n",
        "    k = 5  # Number of similar texts to retrieve\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    print(\"FAISS indices:\", indices)\n",
        "    print(\"FAISS distances:\", distances)\n",
        "\n",
        "    # Retrieve texts using valid indices\n",
        "    retrieved_texts = []\n",
        "    for idx in indices[0]:\n",
        "        if 0 <= idx < len(all_texts):\n",
        "            retrieved_texts.append(all_texts[idx])\n",
        "        else:\n",
        "            print(f\"Warning: Index {idx} is out of bounds for all_texts with length {len(all_texts)}.\")\n",
        "\n",
        "    if not retrieved_texts:\n",
        "        return \"No valid texts retrieved from the index.\"\n",
        "\n",
        "    # Combine the retrieved texts as context\n",
        "    context = \"\\n\".join(retrieved_texts)\n",
        "\n",
        "    # Construct an augmented query with instructions not to restate the context\n",
        "    few_shot_example = (\n",
        "        \"Example 1:\\n\"\n",
        "        \"Q: What are the key challenges in unsupervised domain adaptation in NLP?\\n\"\n",
        "        \"A: First, unsupervised domain adaptation involves transferring knowledge from a labeled source domain \"\n",
        "        \"to an unlabeled target domain. Then, the main challenges include domain shift, scarcity of target labels, \"\n",
        "        \"and potential overfitting to source data. Finally, researchers address these challenges using adversarial \"\n",
        "        \"training, self-supervised methods, and domain-invariant feature extraction.\\n\\n\"\n",
        "    )\n",
        "\n",
        "    augmented_query = (\n",
        "        few_shot_example +\n",
        "        \"The context below is for your reference only.\\n\\n\" +\n",
        "        f\"Context: {context}\\n\\n\" +\n",
        "        \"Question: \" + user_query + \"\\n\\n\" +\n",
        "        \"### Answer:\\n\"\n",
        "    )\n",
        "\n",
        "    # Set up the text-generation pipeline using the 1B Llama model with adjusted parameters\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "        tokenizer=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "        device=0\n",
        "    )\n",
        "\n",
        "    response = generator(\n",
        "        augmented_query,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    generated_text = response[0][\"generated_text\"]\n",
        "\n",
        "    # Extract only the answer portion after the marker\n",
        "    if \"### Answer:\" in generated_text:\n",
        "        answer = generated_text.split(\"### Answer:\")[-1].strip()\n",
        "    else:\n",
        "        answer = generated_text\n",
        "\n",
        "    # Return both the context and the final answer\n",
        "    final_output = f\"Context:\\n{context}\\n\\nAnswer:\\n{answer}\"\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "HgfOqDs_BLX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Optional: Custom CSS to hide any element with a flag attribute (if one still appears)\n",
        "custom_css = \"\"\"\n",
        "button[aria-label=\"Flag\"] {\n",
        "  display: none !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def generate_answer_button(query):\n",
        "    # Simply call your existing function\n",
        "    return generate_answer(query)\n",
        "\n",
        "with gr.Blocks(css=custom_css) as interface:\n",
        "    # Title and description (optional)\n",
        "    gr.Markdown(\"# AI RAG Chatbot\")\n",
        "    gr.Markdown(\"\")\n",
        "\n",
        "    # Input Textbox\n",
        "    query_box = gr.Textbox(\n",
        "        label=\"How may I assist you today\",\n",
        "        lines=2,\n",
        "        placeholder=\"e.g. How to develop a RAG model for LLM\"\n",
        "    )\n",
        "\n",
        "    # Submit Button\n",
        "    submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "    # Output Textbox\n",
        "    answer_box = gr.Textbox(label=\"Generated Answer\")\n",
        "\n",
        "    # Link the button click to the function\n",
        "    submit_button.click(\n",
        "        fn=generate_answer_button,\n",
        "        inputs=query_box,\n",
        "        outputs=answer_box\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "id": "nF0kYyohBAms"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}