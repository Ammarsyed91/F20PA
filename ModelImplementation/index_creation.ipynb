{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu sentence-transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Configure logging for error tracking\n",
    "logging.basicConfig(\n",
    "    filename=\"faiss_errors.log\",\n",
    "    filemode=\"a\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.ERROR\n",
    ")\n",
    "\n",
    "print(\"All packages imported and logging configured!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Load JSONL Files and Prepare Data\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Yield JSON objects from a JSONL file with error logging.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.error(f\"JSON decode error in file {file_path}: {e}\")\n",
    "\n",
    "# Specify the directory containing your JSONL files.\n",
    "jsonl_directory = \"/Users/ammar/Desktop/Dissertation/Dataset\"\n",
    "jsonl_files = glob.glob(os.path.join(jsonl_directory, \"*.jsonl\"))\n",
    "print(f\"Found {len(jsonl_files)} JSONL files in {jsonl_directory}\")\n",
    "\n",
    "# Initialize lists to hold texts and metadata.\n",
    "all_texts = []\n",
    "doc_ids = []\n",
    "chunk_nums = []\n",
    "\n",
    "# Process each JSONL file to extract texts and metadata.\n",
    "for file in jsonl_files:\n",
    "    print(f\"Processing file: {file}\")\n",
    "    for data in load_jsonl(file):\n",
    "        text = data.get(\"text\")  # Assumes the key for text is \"text\"\n",
    "        if text:\n",
    "            # Optional: Truncate text if necessary (e.g., to 4096 bytes)\n",
    "            encoded_text = text.encode(\"utf-8\")\n",
    "            if len(encoded_text) > 4096:\n",
    "                print(f\"Truncating text in file {file}\")\n",
    "                text = encoded_text[:4096].decode(\"utf-8\", errors=\"ignore\")\n",
    "            all_texts.append(text)\n",
    "            doc_ids.append(data.get(\"doc_id\", \"\"))\n",
    "            chunk_nums.append(data.get(\"chunk_num\", 0))\n",
    "\n",
    "print(f\"Loaded {len(all_texts)} text chunks from {len(jsonl_files)} files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Build FAISS Index and Save Files\n",
    "\n",
    "# Load the SentenceTransformer model for generating embeddings.\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model loaded.\")\n",
    "\n",
    "# Define FAISS index parameters.\n",
    "dim = 384  # Dimension for all-MiniLM-L6-v2 embeddings\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "# Process texts in batches to compute embeddings and add them to the FAISS index.\n",
    "batch_size = 1000\n",
    "num_texts = len(all_texts)\n",
    "print(\"Starting batch embedding and FAISS index construction...\")\n",
    "\n",
    "for i in tqdm(range(0, num_texts, batch_size), desc=\"Processing batches\"):\n",
    "    batch_texts = all_texts[i:i+batch_size]\n",
    "    try:\n",
    "        batch_embeddings = embedding_model.encode(batch_texts, show_progress_bar=False)\n",
    "        index.add(np.array(batch_embeddings).astype(\"float32\"))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing batch starting at index {i}: {e}\")\n",
    "\n",
    "print(f\"FAISS index built with {index.ntotal} embeddings.\")\n",
    "\n",
    "# Save the FAISS index and metadata files.\n",
    "faiss.write_index(index, \"my_index.idx\")\n",
    "with open(\"all_texts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_texts, f)\n",
    "with open(\"doc_ids.pkl\", \"wb\") as f:\n",
    "    pickle.dump(doc_ids, f)\n",
    "with open(\"chunk_nums.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunk_nums, f)\n",
    "\n",
    "print(\"FAISS index and metadata files saved!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
